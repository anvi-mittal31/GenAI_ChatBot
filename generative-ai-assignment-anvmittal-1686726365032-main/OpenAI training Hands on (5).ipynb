{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00da6e4a",
   "metadata": {},
   "source": [
    "<b>OpenAI Chat Completion API</b><br>\n",
    "<ul>\n",
    "<li>Chat models take a series of messages as input, and return a model-generated message as output.</li>\n",
    "<li>Useful for single-turn as well as multi-turn conversations</li>\n",
    "    <li>\n",
    "    Roles: system, user, assistant:\n",
    "<ol>\n",
    "<li>User: gives the instructions</li>\n",
    "<li>System: set the behaviour of the assistant</li>\n",
    "<li>Assistant: used to store prior responses and to set the context</li>\n",
    "<li>max_token: The maximum number of tokens to generate in the completion</li>\n",
    "<li>temperature: The sampling temperature to use. Values close to 1 will give the model more risk/creativity, while values close to 0 will generate well-defined answers.</li>\n",
    "<li>n: The number of chat completion choices to generate for each input message.</li></ol></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94ca485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PrettyPrinter.pprint of <pprint.PrettyPrinter object at 0x7fe6442d7640>>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1667a9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenai\u001b[39;00m\n\u001b[1;32m      3\u001b[0m openai\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msk-DFfMtp0Xg55NW8JOvfl2T3BlbkFJcjbImK7Jrw3DvnJpwNjN\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      6\u001b[0m   model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m   messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m      8\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mYou are the HR of a technical firm.\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m      9\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39massistant\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mOn an average people at the firm have received a 10\u001b[39;49m\u001b[39m% r\u001b[39;49;00m\u001b[39maise\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m     10\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mDraft an email informing employees about the raise\u001b[39;49m\u001b[39m\"\u001b[39;49m}\n\u001b[1;32m     11\u001b[0m     ]\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/generative-ai-assignment-anvmittal-1686726365032/GenAIvenv/lib/python3.8/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/generative-ai-assignment-anvmittal-1686726365032/GenAIvenv/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/generative-ai-assignment-anvmittal-1686726365032/GenAIvenv/lib/python3.8/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/generative-ai-assignment-anvmittal-1686726365032/GenAIvenv/lib/python3.8/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/generative-ai-assignment-anvmittal-1686726365032/GenAIvenv/lib/python3.8/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are the HR of a technical firm.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"On an average people at the firm have received a 10% raise\"},\n",
    "        {\"role\": \"user\", \"content\": \"Draft an email informing employees about the raise\"}\n",
    "    ]\n",
    ")\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15088d14",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenai\u001b[39;00m\n\u001b[1;32m      3\u001b[0m query\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput query\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m context_enhanced_query \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39mTask ......\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mContext: \u001b[39m\u001b[39m{\u001b[39;00mcontext\u001b[39m}\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m        Example 1:\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39m        query: example query 1\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[39m        output: example output 1\u001b[39m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[39m        query: \u001b[39m\u001b[39m{\u001b[39;00mquery\u001b[39m}\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m        output:\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     13\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m     14\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     messages \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: context_enhanced_query}],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     presence_penalty\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m output\u001b[39m=\u001b[39mresponse\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'context' is not defined"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "query=\"Input query\"\n",
    "context_enhanced_query = f\"\"\"Task ......\n",
    "Context: {context}\n",
    "        Example 1:\n",
    "        query: example query 1\n",
    "        output: example output 1\n",
    "\n",
    "        query: {query}\n",
    "        output:\"\"\"\n",
    "        \n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages = [{\"role\": \"user\", \"content\": context_enhanced_query}],\n",
    "    temperature=0.3,\n",
    "    max_tokens=1024,\n",
    "    api_key =config('OpenAIKey'),\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    ")\n",
    "output=response.get(\"choices\")[0].get(\"message\").get(\"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90355ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#langchain chat completion\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c8dd997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Subject: Announcement of Raise in Salaries\\n'\n",
      " '\\n'\n",
      " 'Dear Team,\\n'\n",
      " '\\n'\n",
      " 'I am thrilled to announce that our company is doing exceptionally well and '\n",
      " 'we are proud to share the good news with you all. It is my pleasure as the '\n",
      " 'HR of our organization to let you know that we have decided to give a salary '\n",
      " 'raise to all our employees. We appreciate your hard work, dedication, and '\n",
      " 'relentless efforts that make our organization a success.\\n'\n",
      " '\\n'\n",
      " 'Effective from the next payroll cycle, all employees will receive a raise in '\n",
      " 'their salaries. The raise will be determined based on your current role, '\n",
      " 'performance, and market trends. As we know, our talented employees are a '\n",
      " 'significant asset to our company, so it has always been our top priority to '\n",
      " 'make sure they are well-compensated.\\n'\n",
      " '\\n'\n",
      " 'We appreciate all of you for being a part of our team and for contributing '\n",
      " \"to the growth of our organization. We believe that our company's success is \"\n",
      " \"a result of our employees' hard work and dedication, and we want to ensure \"\n",
      " 'that our employees feel valued and appreciated.\\n'\n",
      " '\\n'\n",
      " 'We urge you to continue contributing your best efforts to make this company '\n",
      " 'thrive, and we look forward to your continued commitment and dedication.\\n'\n",
      " '\\n'\n",
      " 'If anyone has further questions or concerns, please feel free to reach out '\n",
      " 'to me or our HR team. We are always here to')\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=1, openai_api_key='OPENAI KEY')\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are the HR of a technical firm\"),\n",
    "    HumanMessage(content=\"Draft an email informing employees about the raise\")\n",
    "]\n",
    "pp.pprint(chat(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2d64f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<b>Embeddings</b><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "471be73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, mode='online'):\n",
    "    if mode =='online':\n",
    "        import time\n",
    "        import openai\n",
    "        EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "        result = openai.Embedding.create(\n",
    "          model=EMBEDDING_MODEL,\n",
    "          input=text,\n",
    "          api_key ='OPENAI KEY'\n",
    "        )\n",
    "        # time.sleep(1)\n",
    "        text_vector = result[\"data\"][0][\"embedding\"]\n",
    "        return text_vector\n",
    "    else:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        # Model 1: msmarco-distilbert-base-v4 \n",
    "        # Model 2: all-MiniLM-L6-v2\n",
    "        model_vector = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "        return model_vector.encode(text)\n",
    "    \n",
    "def get_embeddings(texts, mode='online'):\n",
    "    if mode == 'online':\n",
    "        import numpy\n",
    "        embeddings = [get_embedding(text) for text in texts]\n",
    "        embeddings = numpy.array(embeddings)\n",
    "    else:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        # Model 1: msmarco-distilbert-base-v4 \n",
    "        # Model 2: all-MiniLM-L6-v2\n",
    "        model_vector = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "        embeddings = model_vector.encode(texts)\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "614ee1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_list=[\"hope you are doing well\", \"seems like a pretty boring session\", \"don't worry I'll share the notebook\"]\n",
    "embedding=get_embeddings(text_as_list, mode=\"online\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b0f7d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02518984, -0.01039681,  0.00351508, ..., -0.01099687,\n",
       "        0.01215909, -0.02176635])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb4c8bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59634e9f",
   "metadata": {},
   "source": [
    "<b>Langchain Conversational Memory</b><br>\n",
    "<ul>\n",
    "<li>LLMs are stateless and each query is treated independent of the other</li>\n",
    "<li>Conversational memory enables the llm to answer multiple queries in a chat-like manner</li>\n",
    "    <li>It allows the LLM to remember the previous conversation.</li>\n",
    "    <li>\n",
    "    Types of conversational memory:\n",
    "<ol>\n",
    "<li>ConversationBufferMemory: raw input of the past conversation between the human and AI is passed</li>\n",
    "<li>ConversationSummaryMemory: this form of memory summarizes the conversation history before it is passed to the next prompt</li>\n",
    "<li>ConversationBufferWindowMemory: acts similar to buffer memory but adds a window to the memory(limited history)</li>\n",
    "<li>ConversationSummaryBufferMemory: is a mix of the ConversationSummaryMemory and the ConversationBufferWindowMemory.</li>\n",
    "<li>ConversationKnowledgeGraphMemory</li>\n",
    "<li>ConversationEntityMemory</li></ol></li>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2098113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain openai tiktoken\n",
    "import inspect\n",
    "\n",
    "from getpass import getpass\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chains.conversation.memory import (ConversationBufferMemory, \n",
    "                                                  ConversationSummaryMemory, \n",
    "                                                  ConversationBufferWindowMemory,\n",
    "                                                  ConversationKGMemory)\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import tiktoken\n",
    "\n",
    "llm = OpenAI(\n",
    "    temperature=0, \n",
    "    openai_api_key='OPENAI KEY',\n",
    "    model_name='gpt-3.5-turbo'  # can be used with llms like 'gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result  \n",
    "# memory_types:- ConversationBufferMemory(),ConversationSummaryMemory(llm=llm),ConversationBufferWindowMemory(k=1)\n",
    "conversation_buf = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "\n",
    "# till End\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78d32d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 108 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to your personal information, so I don't know your name. However, I'm happy to chat with you and learn more about you!\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf,\n",
    "    \"Do you know my name?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f2e6a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 158 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Nabeel! Is there anything specific you'd like to talk about or learn from me? I have a wide range of knowledge and can provide information on various topics.\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf,\n",
    "    \"My Name is Nabeel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "855cf506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 202 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Yes, I do know your name now, Nabeel! Thank you for sharing it with me earlier. Is there anything else you'd like to chat about?\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf,\n",
    "    \"Now do you know my name?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f07882cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Do you know my name?\n",
      "AI: I'm sorry, but I don't have access to your personal information, so I don't know your name. However, I'm happy to chat with you and learn more about you!\n",
      "Human: My Name is Nabeel\n",
      "AI: Nice to meet you, Nabeel! Is there anything specific you'd like to talk about or learn from me? I have a wide range of knowledge and can provide information on various topics.\n",
      "Human: Now do you know my name?\n",
      "AI: Yes, I do know your name now, Nabeel! Thank you for sharing it with me earlier. Is there anything else you'd like to chat about?\n"
     ]
    }
   ],
   "source": [
    "print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b11855df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation_buf.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11dc6f1",
   "metadata": {},
   "source": [
    "<b>Langchain agents</b><br>\n",
    "<ul><li>Agents are like external tools for LLMs. Agents allow a LLM to access Google search, perform complex calculations with Python, and even make SQL queries.</li>\n",
    "    <li>Agents gives LLM's the possibility of using tools in their workflow</li>\n",
    "    <li>\n",
    "    Using one of langchain's pre-built agents involves three variables:\n",
    "<ol>\n",
    "<li>defining the tools</li>\n",
    "<li>defining the llm</li>\n",
    "<li>defining the agent type</li></ol></li>\n",
    "    <li>While working with agents we must initialize the max_iterations paramemter as there's a possibility that agent might get stuck in an infinite loop, default value of the parameter is 15</li>\n",
    "    <li>We'll be working with 'Self Ask with Search' type of Agent </li>\n",
    "    <li>Useful in scenarios where we to extract information using a search engine</li>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ffe14cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    openai_api_key=\"OPENAI KEY\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7af5b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def count_tokens(agent, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = agent(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f5299bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, SerpAPIWrapper\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "search = SerpAPIWrapper(serpapi_api_key='SERP API KEY')\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description='google search'\n",
    "    )\n",
    "]\n",
    "\n",
    "self_ask_with_search = initialize_agent(tools, llm, agent=\"self-ask-with-search\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "94cb565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(query):    \n",
    "    response = count_tokens(\n",
    "        self_ask_with_search, \n",
    "        f\"\"\"{query}\"\"\"\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "db55f440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Yes.\n",
      "Follow up: What year did World War 2 start?\u001b[0m\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3mSeptember 1, 1939 â€“ September 2, 1945\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mSo the final answer is: September 1, 1939\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 865 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'when was the world war 2 started?', 'output': 'September 1, 1939'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_web('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce86f1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "Question: When was the founder of craigslist born?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the founder of craigslist?\n",
      "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
      "Follow up: When was Craig Newmark born?\n",
      "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
      "So the final answer is: December 6, 1952\n",
      "\n",
      "Question: Who was the maternal grandfather of George Washington?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n",
      "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who is the director of Jaws?\n",
      "Intermediate answer: The director of Jaws is Steven Spielberg.\n",
      "Follow up: Where is Steven Spielberg from?\n",
      "Intermediate answer: The United States.\n",
      "Follow up: Who is the director of Casino Royale?\n",
      "Intermediate answer: The director of Casino Royale is Martin Campbell.\n",
      "Follow up: Where is Martin Campbell from?\n",
      "Intermediate answer: New Zealand.\n",
      "So the final answer is: No\n",
      "\n",
      "Question: {input}\n",
      "Are followup questions needed here:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(self_ask_with_search.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4e27bb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==1.1.0\n",
      "aiohttp==3.8.4\n",
      "aiosignal==1.3.1\n",
      "anyio==3.5.0\n",
      "argilla==1.3.1\n",
      "argon2-cffi==21.3.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "astroid==2.15.0\n",
      "asttokens==2.0.5\n",
      "astunparse==1.6.3\n",
      "async-timeout==4.0.2\n",
      "atomicwrites==1.4.0\n",
      "attrs==21.4.0\n",
      "Babel==2.9.1\n",
      "backcall==0.2.0\n",
      "backoff==2.2.1\n",
      "beautifulsoup4==4.11.1\n",
      "bleach==5.0.0\n",
      "blis==0.7.9\n",
      "blobfile==2.0.1\n",
      "cachetools==4.2.4\n",
      "catalogue==2.0.8\n",
      "certifi==2022.12.7\n",
      "cffi==1.15.0\n",
      "charset-normalizer==2.0.12\n",
      "click==8.1.3\n",
      "cloudpickle==2.1.0\n",
      "colorama==0.4.6\n",
      "commonmark==0.9.1\n",
      "confection==0.0.4\n",
      "coverage==6.4\n",
      "cryptography==39.0.1\n",
      "cycler==0.11.0\n",
      "cymem==2.0.7\n",
      "Cython==0.29.33\n",
      "dataclasses-json==0.5.7\n",
      "debugpy==1.6.0\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "Deprecated==1.2.13\n",
      "dill==0.3.6\n",
      "distlib==0.3.6\n",
      "docstring-parser==0.14.1\n",
      "docutils==0.18.1\n",
      "docx==0.2.4\n",
      "en-core-web-lg @ https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl\n",
      "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl\n",
      "entrypoints==0.4\n",
      "et-xmlfile==1.1.0\n",
      "executing==0.8.3\n",
      "faiss-cpu==1.7.3\n",
      "fastjsonschema==2.15.3\n",
      "filelock==3.9.0\n",
      "fire==0.4.0\n",
      "flatbuffers==1.12\n",
      "fonttools==4.33.3\n",
      "frozenlist==1.3.3\n",
      "funcsigs==1.0.2\n",
      "gast==0.4.0\n",
      "google==3.0.0\n",
      "google-api-core==2.8.1\n",
      "google-api-python-client==1.12.11\n",
      "google-auth==1.35.0\n",
      "google-auth-httplib2==0.1.0\n",
      "google-auth-oauthlib==0.4.6\n",
      "google-cloud==0.34.0\n",
      "google-cloud-aiplatform==1.15.0\n",
      "google-cloud-bigquery==2.34.4\n",
      "google-cloud-core==2.3.1\n",
      "google-cloud-notebooks==1.3.2\n",
      "google-cloud-pipeline-components==1.0.12\n",
      "google-cloud-resource-manager==1.5.1\n",
      "google-cloud-storage==1.44.0\n",
      "google-cloud-vision==2.7.3\n",
      "google-crc32c==1.3.0\n",
      "google-pasta==0.2.0\n",
      "google-resumable-media==2.3.3\n",
      "google-search-results==2.4.2\n",
      "googleapis-common-protos==1.56.2\n",
      "greenlet==2.0.2\n",
      "grpc-google-iam-v1==0.12.4\n",
      "grpcio==1.46.3\n",
      "grpcio-status==1.46.3\n",
      "h11==0.14.0\n",
      "h5py==3.7.0\n",
      "httpcore==0.16.3\n",
      "httplib2==0.20.4\n",
      "httpx==0.23.3\n",
      "huggingface-hub==0.12.1\n",
      "idna==3.4\n",
      "importlib-metadata==4.11.4\n",
      "iniconfig==1.1.1\n",
      "ipykernel==6.13.0\n",
      "ipython==7.33.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.7.0\n",
      "isort==5.12.0\n",
      "jedi==0.18.1\n",
      "Jinja2==3.1.1\n",
      "joblib==1.1.0\n",
      "json5==0.9.6\n",
      "jsonschema==3.2.0\n",
      "jupyter==1.0.0\n",
      "jupyter-client==7.2.2\n",
      "jupyter-console==6.4.3\n",
      "jupyter-core==4.10.0\n",
      "jupyter-server==1.16.0\n",
      "jupyterlab==3.3.4\n",
      "jupyterlab-pygments==0.2.2\n",
      "jupyterlab-server==2.13.0\n",
      "jupyterlab-widgets==1.1.0\n",
      "keras==2.9.0\n",
      "Keras-Preprocessing==1.1.2\n",
      "keyring==23.5.1\n",
      "kfp==1.8.12\n",
      "kfp-pipeline-spec==0.1.16\n",
      "kfp-server-api==1.8.2\n",
      "kiwisolver==1.4.2\n",
      "kubernetes==18.20.0\n",
      "langchain==0.0.107\n",
      "langcodes==3.3.0\n",
      "lazy-object-proxy==1.9.0\n",
      "libclang==14.0.1\n",
      "lxml==4.9.2\n",
      "Markdown==3.4.1\n",
      "MarkupSafe==2.1.1\n",
      "marshmallow==3.19.0\n",
      "marshmallow-enum==1.5.1\n",
      "matplotlib==3.5.2\n",
      "matplotlib-inline==0.1.3\n",
      "mccabe==0.7.0\n",
      "mistune==0.8.4\n",
      "monotonic==1.6\n",
      "MouseInfo==0.1.3\n",
      "multidict==6.0.4\n",
      "murmurhash==1.0.9\n",
      "mypy-extensions==1.0.0\n",
      "nbclassic==0.3.7\n",
      "nbclient==0.6.0\n",
      "nbconvert==6.5.0\n",
      "nbformat==5.3.0\n",
      "nest-asyncio==1.5.5\n",
      "nltk==3.8.1\n",
      "notebook==6.4.11\n",
      "notebook-shim==0.1.0\n",
      "numpy==1.23.5\n",
      "oauth2client==4.1.3\n",
      "oauthlib==3.2.0\n",
      "openai==0.27.4\n",
      "openapi-schema-pydantic==1.2.4\n",
      "opencv-python==4.7.0.68\n",
      "openpyxl==3.1.1\n",
      "opt-einsum==3.3.0\n",
      "packaging==21.3\n",
      "pandas==1.4.2\n",
      "pandocfilters==1.5.0\n",
      "parso==0.8.3\n",
      "pathy==0.10.1\n",
      "pdfminer.six==20221105\n",
      "pdfplumber==0.8.0\n",
      "pickleshare==0.7.5\n",
      "Pillow==9.1.1\n",
      "pkginfo==1.8.2\n",
      "platformdirs==3.0.0\n",
      "plotly==4.14.3\n",
      "pluggy==1.0.0\n",
      "preshed==3.0.8\n",
      "prometheus-client==0.14.1\n",
      "prompt-toolkit==3.0.29\n",
      "proto-plus==1.20.6\n",
      "protobuf==3.19.4\n",
      "psutil==5.9.0\n",
      "pure-eval==0.2.2\n",
      "py==1.11.0\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "PyAutoGUI==0.9.53\n",
      "pycparser==2.21\n",
      "pycryptodomex==3.17\n",
      "pydantic==1.9.1\n",
      "PyGetWindow==0.0.9\n",
      "Pygments==2.11.2\n",
      "pylint==2.17.0\n",
      "PyMsgBox==1.0.9\n",
      "pyparsing==3.0.8\n",
      "pyperclip==1.8.2\n",
      "PyRect==0.2.0\n",
      "pyrsistent==0.18.1\n",
      "PyScreeze==0.1.28\n",
      "pytest==7.1.2\n",
      "pytest-cov==2.12.1\n",
      "python-dateutil==2.8.2\n",
      "python-docx==0.8.11\n",
      "python-magic==0.4.27\n",
      "python-pptx==0.6.21\n",
      "pytweening==1.0.4\n",
      "pytz==2022.1\n",
      "pywin32==303\n",
      "pywin32-ctypes==0.2.0\n",
      "pywinpty==2.0.5\n",
      "PyYAML==6.0\n",
      "pyzmq==22.3.0\n",
      "qtconsole==5.3.0\n",
      "QtPy==2.0.1\n",
      "readme-renderer==35.0\n",
      "recmetrics==0.1.5\n",
      "regex==2022.10.31\n",
      "requests==2.27.1\n",
      "requests-oauthlib==1.3.1\n",
      "requests-toolbelt==0.9.1\n",
      "retrying==1.3.3\n",
      "rfc3986==1.5.0\n",
      "rich==12.4.4\n",
      "rsa==4.8\n",
      "scikit-learn==1.0.2\n",
      "scipy==1.8.0\n",
      "seaborn==0.11.2\n",
      "Send2Trash==1.8.0\n",
      "sentence-transformers==2.2.2\n",
      "sentencepiece==0.1.97\n",
      "six==1.16.0\n",
      "sklearn==0.0\n",
      "smart-open==6.3.0\n",
      "sniffio==1.2.0\n",
      "soupsieve==2.3.2.post1\n",
      "spacy==3.5.0\n",
      "spacy-legacy==3.0.12\n",
      "spacy-loggers==1.0.4\n",
      "SQLAlchemy==1.4.46\n",
      "srsly==2.4.6\n",
      "stack-data==0.2.0\n",
      "strip-hints==0.1.10\n",
      "tabulate==0.8.10\n",
      "tenacity==8.2.2\n",
      "tensorboard==2.9.1\n",
      "tensorboard-data-server==0.6.1\n",
      "tensorboard-plugin-wit==1.8.1\n",
      "tensorflow-estimator==2.9.0\n",
      "tensorflow-io-gcs-filesystem==0.26.0\n",
      "termcolor==1.1.0\n",
      "terminado==0.13.3\n",
      "thinc==8.1.8\n",
      "threadpoolctl==3.1.0\n",
      "tika==2.6.0\n",
      "tiktoken==0.2.0\n",
      "tinycss2==1.1.1\n",
      "tokenizers==0.13.2\n",
      "toml==0.10.2\n",
      "tomli==2.0.1\n",
      "tomlkit==0.11.6\n",
      "torch==1.13.1\n",
      "torchvision==0.14.1\n",
      "tornado==6.1\n",
      "tqdm==4.64.1\n",
      "traitlets==5.1.1\n",
      "transformers==4.26.1\n",
      "twine==4.0.0\n",
      "typer==0.4.1\n",
      "typing-inspect==0.8.0\n",
      "typing_extensions==4.2.0\n",
      "unstructured==0.5.2\n",
      "uritemplate==3.0.1\n",
      "urllib3==1.26.14\n",
      "virtualenv==20.20.0\n",
      "Wand==0.6.11\n",
      "wasabi==1.1.1\n",
      "wcwidth==0.2.5\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.3.2\n",
      "Werkzeug==2.2.1\n",
      "widgetsnbextension==3.6.0\n",
      "wikipedia==1.4.0\n",
      "wrapt==1.14.1\n",
      "XlsxWriter==3.0.8\n",
      "yarl==1.8.2\n",
      "zipp==3.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f2ab1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
